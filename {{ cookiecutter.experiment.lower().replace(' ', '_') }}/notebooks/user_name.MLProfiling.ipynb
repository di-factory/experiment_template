{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML_PROFILING NOTEBOOK\n",
    "## It's for doing the ml profiling once the data is analized in DATA_PROFILING NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# The %load_ext autoreload and %autoreload 2 magic commands are used to automatically \n",
    "# reload modules when they are changed. This can be useful when you are developing code \n",
    "# in an interactive environment, as it allows you to see the changes you make to your modules \n",
    "# without having to restart the kernel.\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the necesary libraries and print the acces to conf file config.yaml\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import hydra\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "import os\n",
    "\n",
    "\n",
    "# for global initialization: NOT RECOMMENDED\n",
    "#initialize(version_base=None, config_path=\"../src/conf\")\n",
    "#compose(config_name='config')\n",
    "\n",
    "print(\"\\n\\nthis shows the cfg file:\")\n",
    "with initialize(version_base=None, config_path=\"../src/conf\"):\n",
    "    cfg = compose(config_name='config')\n",
    "    print(cfg)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the dataset\n",
    "data = pd.read_csv(os.path.join('../data/raw', cfg.file_names.raw_file), \n",
    "                   encoding=cfg.general_ml.encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And finally choose the Ml model to be applied, among: regression, Classifications, time_series, Clustering, NLP\n",
    "#from pycaret.classification import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findind the most accurate subdataset\n",
    "### which would be the best models with whole trainning data? \n",
    "Once finding, get the three bes and using plot_model with 'feature', find the most reevant variables to use, and cut the entire dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can see below the different considerations about model setup, and correct whatever you need to\n",
    "\n",
    "#selected_models = setup( data = data, \n",
    "#                        target = cfg.data_fields.label, session_id=cfg.general_ml.seed,\n",
    "#                        transformation=True,\n",
    "#                        fix_imbalance = True, #8:2\n",
    "#                        normalize=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let`s comapre different models and define the best for this kind of experiment, using F1 (Because Imbalanmce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_from_selection= compare_models(sort='F1') #F1: because it's an imbalance model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case, we choose lr, dt and ada as models to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to create feature plots from selected, to check which subset of fields are the most relevant\n",
    "\n",
    "#selected=['lr', 'dt', 'ada'] # use your own selection\n",
    "#for sel in selected:\n",
    "#    print(str(sel)) \n",
    "#    plot_model(create_model(sel), plot='feature', use_train_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so seeing the results before.. this is the selection of the best fields:\n",
    "\n",
    "#best_features = ['dec_o', 'dec', 'like', 'attr', 'from', 'zipcode', 'prob', 'like_o', 'fun', 'undergra', \n",
    "#                 cfg.data_fields.label] #this ast one is the target field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping the setup with this fields selection and get the new set of bestmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_data = data[best_features]\n",
    "#selected_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reserving test data\n",
    "\n",
    "#test_data = selected_data.sample(frac= float(cfg.data_pipeline.data_transform_params.percent_test))\n",
    "#selected_data = selected_data.drop(test_data.index, axis = 0)\n",
    "#selected_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup mlflow server in s3\n",
    "\n",
    "# this code never runs standalone, it must be tested in aws\n",
    "\n",
    "#import mlflow\n",
    "#mlflow.set_tracking_uri('ec2-3-140-190-151.us-east-2.compute.amazonaws.com:5000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and let's review that the selected fields got the same results\n",
    "\n",
    "# match_improved = setup(data = selected_data, #see above \n",
    "#                        log_experiment = True,\n",
    "#                        experiment_name = f'{cfg.general_ml.client}-{cfg.general_ml.project}-{cfg.general_ml.experiment}',\n",
    "#                        target = cfg.data_fields.label, # get the target label from cfg\n",
    "#                        session_id=cfg.general_ml.seed, # get the seed from config\n",
    "#                        train_size = 1.0-float(cfg.data_pipeline.data_transform_params.percent_valid), #get %valid from cfg\n",
    "#                        transformation=True, \n",
    "#                        fix_imbalance = True, #8:2\n",
    "#                        normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "\n",
    "# best_model = compare_models(sort='F1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok seems that everything is ok, so print best model\n",
    "\n",
    "#print (best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And create the selected model in ten kfolds\n",
    "\n",
    "#selected_model = create_model('lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets find the best hyperparameters for the selected model \n",
    "\n",
    "tuned_model = tune_model(selected_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and .. evaluate the model\n",
    "\n",
    "plot_model()\n",
    "evaluate_model(tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Finally trains a the model on the entire dataset including the hold-out set.\n",
    "\n",
    "final_model = finalize_model(tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "\n",
    "save_model(final_model, os.path.join(cfg.paths.models, cfg.file_names.ml_profiling_model))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, once developed a protoype model in this notebook, it`s time to transforming this experiment in a production model. So let's use some different utils tha can serve to this goal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getback the model to make predictions\n",
    "\n",
    "saved_model = load_model(os.path.join(cfg.paths.models, cfg.file_names.ml_profiling_model));\n",
    "saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions***\n",
    "\n",
    "#prediction = predict_model(saved_model,data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets print in text the pipeline to be used :\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(display = 'text')\n",
    "get_config('pipeline')\n",
    "\n",
    "# ***** be careful to review that this is the whole pipeline, and copy/paste in pipeline.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This print_pycaret_pipeline() function, is useful to identifiy the \n",
    "# different commponents of the pipeline and the libraries to be imported\n",
    "# https://openscoring.io/blog/2023/01/12/converting_sklearn_pycaret3_pipeline_pmml/\n",
    "\n",
    "from pycaret.internal.preprocess.transformers import TransformerWrapper\n",
    "from pycaret.internal.pipeline import Pipeline as PyCaretPipeline\n",
    "try: \n",
    "    from sklearn2pmml.util import fqn\n",
    "except error:\n",
    "    !pip install sklearn2pmml    \n",
    "    from sklearn2pmml.util import fqn\n",
    "\n",
    "def print_pycaret_pipeline(pipeline):\n",
    "  if not isinstance(pipeline, PyCaretPipeline):\n",
    "    raise TypeError()\n",
    "  steps = pipeline.steps\n",
    "  transformer_steps = steps[:-1]\n",
    "  final_estimator_step = steps[-1]\n",
    "\n",
    "  for transformer_step in transformer_steps:\n",
    "    name = transformer_step[0]\n",
    "    transformer = transformer_step[1]\n",
    "    if not isinstance(transformer, TransformerWrapper):\n",
    "      raise TypeErrpr()\n",
    "    print(\"{} -> {} // {} inputs\".format(name, fqn(transformer.transformer), len(transformer._include)))\n",
    "    #put away the len to see the different fields if needed\n",
    "\n",
    "  name = final_estimator_step[0]\n",
    "  final_estimator = final_estimator_step[1]\n",
    "  print(\"{} -> {}\".format(name, fqn(final_estimator)))\n",
    "\n",
    "print_pycaret_pipeline(saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_config? \n",
    "# set_config?\n",
    "# to consult/update the different hyperparameters of the experiments\n",
    "# for example:\n",
    "get_config('y_transformed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
